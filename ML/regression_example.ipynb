{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://deeplearningcourses.com/c/deep-learning-prerequisites-the-numpy-stack-in-python\n",
    "# https://www.udemy.com/deep-learning-prerequisites-the-numpy-stack-in-python\n",
    "# YouTube direct link: http://bit.ly/2LENC50\n",
    "\n",
    "# Get the data from:\n",
    "# https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just in case we need it\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# important note: this is where we will usually put data files\n",
    "df = pd.read_csv('../large_files/airfoil_self_noise.dat', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503 entries, 0 to 1502\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       1503 non-null   int64  \n",
      " 1   1       1503 non-null   float64\n",
      " 2   2       1503 non-null   float64\n",
      " 3   3       1503 non-null   float64\n",
      " 4   4       1503 non-null   float64\n",
      " 5   5       1503 non-null   float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 70.6 KB\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "df.head()\n",
    "df.info()\n",
    "\n",
    "# get the inputs\n",
    "data = df[[0,1,2,3,4]].values\n",
    "\n",
    "# get the outputs\n",
    "target = df[5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normally we would put all of our imports at the top\n",
    "# but this lets us tell a story\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "# this lets us simulate how our model will perform in the future\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a classifer and train it\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5178174085574289\n",
      "0.5093302963242687\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model's performance\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([133.09103598, 128.09463829, 114.23583955, 130.71720728,\n",
       "       126.87377129, 127.80461105, 114.64524473, 122.20951344,\n",
       "       122.15713823, 132.54714837, 122.78963455, 130.64543105,\n",
       "       130.41579885, 121.01610784, 123.36819134, 122.21911935,\n",
       "       129.3668704 , 133.21593625, 127.6987309 , 125.77511823,\n",
       "       128.34871566, 124.22591775, 114.73113592, 126.23404937,\n",
       "       126.14876802, 131.91809122, 122.01934171, 127.65090425,\n",
       "       129.7628116 , 128.29778733, 118.26317918, 123.26405122,\n",
       "       129.52756453, 131.32563954, 122.4765458 , 124.67222433,\n",
       "       131.06754192, 126.75148746, 127.90954938, 120.57014736,\n",
       "       119.81027742, 125.50628252, 125.82774719, 129.26535996,\n",
       "       119.2573937 , 133.12921143, 118.50082589, 123.38691543,\n",
       "       125.63707539, 120.34808467, 127.63975341, 125.51029424,\n",
       "       128.06546884, 117.77263211, 116.79691774, 126.1258583 ,\n",
       "       129.60193423, 124.93801031, 124.52513098, 125.34133076,\n",
       "       120.14249289, 123.98722809, 131.00224314, 118.23810628,\n",
       "       118.82110504, 120.05756699, 132.70542609, 121.9716836 ,\n",
       "       122.90999182, 122.59031597, 119.80514229, 129.96718616,\n",
       "       125.0473533 , 112.21293692, 120.91505703, 124.52625699,\n",
       "       126.01565012, 127.27352522, 128.37824481, 112.85173032,\n",
       "       120.33818194, 121.58564649, 125.48915223, 135.41437109,\n",
       "       127.61415337, 130.10292361, 125.84016773, 123.41778212,\n",
       "       128.02183595, 130.22565502, 119.11439053, 122.38847581,\n",
       "       120.22497946, 127.37960823, 123.92927923, 128.79626969,\n",
       "       124.91899083, 122.61878576, 122.26820371, 123.08086304,\n",
       "       124.83420614, 125.35454686, 121.87939471, 126.63961945,\n",
       "       127.82087496, 124.7455956 , 120.4921896 , 124.17368521,\n",
       "       116.57601981, 120.19863542, 128.30836535, 125.58983847,\n",
       "       125.12593906, 123.21151584, 121.68127745, 121.29622031,\n",
       "       134.18459389, 119.67773807, 132.22800483, 118.90850312,\n",
       "       127.22057773, 130.12755733, 122.29858305, 130.35486383,\n",
       "       123.99670262, 131.03416017, 123.32943246, 132.61145261,\n",
       "       126.31358352, 121.09335155, 121.69218999, 125.89620574,\n",
       "       124.11570154, 126.15624794, 111.98506999, 118.22417482,\n",
       "       122.27290402, 121.76623847, 128.31313433, 112.90335162,\n",
       "       120.64961751, 126.96243706, 130.89063027, 128.52896057,\n",
       "       115.55521342, 128.2202356 , 129.76167713, 128.56529798,\n",
       "       120.98199044, 131.84394505, 129.29957432, 128.10354848,\n",
       "       124.84080635, 118.68450962, 125.47496581, 126.27529931,\n",
       "       126.36636837, 120.03208627, 121.87154685, 118.95076895,\n",
       "       124.89105236, 132.23584475, 127.38539323, 123.59458452,\n",
       "       126.48595572, 133.08398491, 127.14912882, 129.94670858,\n",
       "       122.36116309, 129.21053521, 125.66616137, 113.2895304 ,\n",
       "       128.14881034, 114.06348549, 121.06944264, 128.06129192,\n",
       "       119.27412081, 130.79977429, 128.67889704, 127.79890064,\n",
       "       125.94793172, 122.89712603, 124.7592834 , 126.66358066,\n",
       "       133.31457908, 115.4389747 , 123.54835006, 126.5310891 ,\n",
       "       117.05631616, 127.56359554, 125.69499781, 122.01840559,\n",
       "       132.39185875, 122.35391505, 126.2442085 , 117.18086781,\n",
       "       132.76370576, 116.64923099, 128.67050268, 123.60844867,\n",
       "       122.04154091, 121.48911506, 121.66386674, 126.88905095,\n",
       "       123.20725821, 120.87361102, 126.77641519, 126.25514518,\n",
       "       122.12029781, 125.85103126, 121.95928431, 126.89318176,\n",
       "       126.47652749, 117.59425433, 109.29592404, 127.577239  ,\n",
       "       125.68579635, 114.23108964, 123.02238158, 129.92665939,\n",
       "       117.28907072, 124.78773708, 130.25043298, 125.63848912,\n",
       "       130.63706796, 122.6570952 , 123.91766678, 126.02868054,\n",
       "       123.11460197, 118.53285996, 127.14377963, 124.88293409,\n",
       "       121.65001361, 124.49490739, 128.73535065, 129.86503717,\n",
       "       112.92202816, 131.76675448, 130.94480084, 122.0830213 ,\n",
       "       119.50858712, 126.43218667, 132.80071068, 132.23019548,\n",
       "       129.03438825, 127.40030638, 128.75040416, 123.87340092,\n",
       "       122.70454389, 121.51221709, 128.4793168 , 120.45109599,\n",
       "       126.42874958, 119.52599783, 127.68500298, 121.8379589 ,\n",
       "       126.33267944, 126.72789193, 124.6559209 , 126.97046859,\n",
       "       129.00401983, 122.99422267, 121.7453059 , 129.69238174,\n",
       "       122.43061474, 126.7870681 , 133.0435635 , 126.68153661,\n",
       "       118.59653956, 123.37622286, 122.10035274, 129.98990211,\n",
       "       120.9417019 , 130.90384199, 122.95831418, 130.84164835,\n",
       "       131.66806594, 122.91559871, 131.55195059, 121.17914573,\n",
       "       117.83086331, 125.40340116, 125.12995077, 128.77785673,\n",
       "       111.72905451, 127.96331985, 113.68825293, 124.64049573,\n",
       "       127.75542322, 126.09398616, 120.50181557, 122.80222784,\n",
       "       120.85391468, 120.53782082, 121.18574259, 132.27005469,\n",
       "       127.20086457, 132.64025087, 126.74277735, 127.27564361,\n",
       "       121.87624312, 123.03743866, 119.33614744, 122.00982009,\n",
       "       122.53473406, 132.13361919, 120.3664864 , 131.7615373 ,\n",
       "       111.2722172 , 123.65540255, 123.16066112, 121.88786025,\n",
       "       130.67514298, 129.87638103, 120.09715089, 121.23425814,\n",
       "       136.06163787, 129.42584041, 125.36272487, 124.55943558,\n",
       "       120.77372937, 126.81655207, 127.18753522, 131.28772248,\n",
       "       121.63928726, 125.74326728, 117.30261394, 124.617358  ,\n",
       "       129.28964749, 120.19203138, 116.76323774, 126.50342036,\n",
       "       125.34547874, 131.96255684, 125.80276179, 117.45522015,\n",
       "       126.74117399, 121.1352068 , 127.7599517 , 127.70021691,\n",
       "       127.65386869, 114.51982752, 120.45859958, 121.35560212,\n",
       "       123.29881452, 126.44036468, 120.97932972, 131.50365155,\n",
       "       123.274541  , 130.30685499, 120.60532293, 127.87429854,\n",
       "       122.91782516, 131.18669866, 125.7850648 , 126.10989541,\n",
       "       120.95816549, 129.70126282, 116.69624381, 125.54484603,\n",
       "       123.77326417, 124.22134069, 120.36544806, 129.43374888,\n",
       "       122.9784681 , 130.16103777, 127.95229321, 122.94805282,\n",
       "       127.29748643, 119.28166071, 128.87654045, 126.85467308,\n",
       "       121.13216265, 128.02477532, 116.19842398, 131.59720337,\n",
       "       126.80576156, 123.78627606, 130.23584   , 135.19884312,\n",
       "       127.63445821, 132.04093741, 121.8458968 , 124.08673531,\n",
       "       124.73304236, 124.72945253, 122.89207368, 120.00823463,\n",
       "       112.61171123, 125.67967775, 120.55880485, 129.02157881,\n",
       "       125.74483937, 130.85706287, 130.87751805, 126.28865074,\n",
       "       130.46303245, 123.6947341 , 133.22735686, 126.0139363 ,\n",
       "       115.5291062 , 132.29843469, 104.3122593 , 124.62168855,\n",
       "       119.49198636, 130.8930864 , 117.78221186, 120.51022088,\n",
       "       121.72458803, 122.27875534, 129.59174925, 120.4595246 ,\n",
       "       120.35645059, 122.64746922, 127.11043326, 127.37770695,\n",
       "       127.04633198, 133.85347078, 127.26814736, 124.54544318,\n",
       "       117.87680665, 116.48441549, 126.36664667, 130.7546291 ,\n",
       "       122.62897074, 125.99080053, 128.45941783, 125.30444236,\n",
       "       123.41422342, 127.74473388, 127.47493791, 122.00745553,\n",
       "       121.03077151, 127.50186092, 120.33503351, 126.12322476,\n",
       "       130.94090862, 129.25752878, 124.63178847, 126.21478107,\n",
       "       128.8697689 , 120.70387206, 122.56944301, 124.77406539,\n",
       "       125.29486169, 121.04057324, 123.93207439, 129.01215058,\n",
       "       131.39634819, 127.71480989, 110.96501328, 131.49922048,\n",
       "       124.09075725, 126.65689266, 134.80070068, 125.7800102 ,\n",
       "       115.7095576 , 119.94959302, 128.60618347, 127.50638939,\n",
       "       116.56305177, 124.81186028, 125.09247523, 122.59319008,\n",
       "       129.1722781 , 120.56962617, 127.21479273, 129.06100972,\n",
       "       128.11247162, 116.88377613, 125.84340078, 124.91483476,\n",
       "       127.52920592, 124.54375543, 128.90949727, 132.62369358,\n",
       "       124.66949911, 116.96794091, 126.68945439, 127.25626417,\n",
       "       121.41470955, 126.30911093, 122.04421021, 130.08561748,\n",
       "       122.71889226, 128.98750626, 134.51259935, 122.50274885,\n",
       "       132.219486  , 128.32969162, 123.11825289, 125.1878732 ,\n",
       "       134.09652998, 115.68413171, 126.59109776, 124.35267605])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how you can make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# what did we get?\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even use random forest to solve the same problem!\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = RandomForestRegressor()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98918106208871\n",
      "0.919442618951517\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model's performance\n",
    "print(model2.score(X_train, y_train))\n",
    "print(model2.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even use deep learning to solve the same problem!\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll learn why scaling is needed in a later course\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train)\n",
    "X_test2 = scaler.transform(X_test)\n",
    "scaler2 = StandardScaler()\n",
    "y_train2 = scaler2.fit_transform(np.expand_dims(y_train, -1)).ravel()\n",
    "y_test2 = scaler2.fit_transform(np.expand_dims(y_test, -1)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor(max_iter=500)\n",
    "model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8458209973424983\n",
      "0.8067769433655068\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model's performance\n",
    "print(model.score(X_train2, y_train2))\n",
    "print(model.score(X_test2, y_test2))\n",
    "# not as good as a random forest!\n",
    "# but not as bad as linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
